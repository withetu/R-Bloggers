---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

```{r}
#========================================================================================================
#https://www.r-bloggers.com/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know/
#Text mining and word cloud fundamentals in R : 5 simple steps you should know
#========================================================================================================

library(tm) # for text mining
library(SnowballC) # for text stemming
library(wordcloud) # word-cloud generator
library(RColorBrewer) # color palettes

#To import the file saved locally in your computer, type the following R code. You will be asked to choose the text file interactively.
text <- readLines(file.choose()) 
text <- readLines("C:\\Users\\user\\Documents\\GitHub\\R-Bloggers\\text-mining.txt")

#Load the data as a corpus
docs <- Corpus(VectorSource(text)) #VectorSource() function creates a corpus of character vectors

#Inspect the content of the document
inspect(docs)

                                  #Text transformation
#Transformation is performed using tm_map() function to replace, for example, special characters from the text.
#Replacing “/”, “@” and “|” with space:
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

                                  #Cleaning the text
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

                                  #Build a term-document matrix
#Document matrix is a table containing the frequency of the words. Column names are words and row names are documents.
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word  = names(v), freq = v)
head(d,10)

                                  #Generate the Word cloud
#The importance of words can be illustrated as a word cloud as follow :
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
#The above word cloud clearly shows that “adi”, “aluminium”, “cast”, “steel” and “high” are the five most important words in the interview
#words : the words to be plotted
#freq : their frequencies
#min.freq : words with frequency below min.freq will not be plotted
#max.words : maximum number of words to be plotted
#random.order : plot words in random order. If false, they will be plotted in decreasing frequency
#rot.per : proportion words with 90 degree rotation (vertical text)
#colors : color words from least to most frequent. Use, for example, colors =“black” for single color.


                                  #Explore frequent terms and their associations

#You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least ten times :
findFreqTerms(dtm, lowfreq = 10)
#You can analyze the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with “freedom” in I have a dream speech :
findAssocs(dtm, terms = "aluminium", corlimit = 0.3)

#The frequency table of words
head(d, 10)
#Plot word frequencies (first 10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word, 
        col = "lightblue", main = "Most Frequent Words",
        ylab = "Word Frequencies")





##=====================AlCIrcle Use=====================##



text <- readLines(file.choose()) 
text <- readLines("C:\\Users\\user\\Documents\\GitHub\\R-Bloggers\\text-mining-alc.txt")

docs <- Corpus(VectorSource(text)) #VectorSource() function creates a corpus of character vectors

inspect(docs)

                                  #Text transformation
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

                                  #Cleaning the text
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("blabla1", "blabla2"))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Text stemming
docs <- tm_map(docs, stemDocument)

                                  #Build a term-document matrix
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = TRUE)
d <- data.frame(word  = names(v), freq = v)
head(d,10)

                                  #Generate the Word cloud

set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))


                                  #Explore frequent terms and their associations


findFreqTerms(dtm, lowfreq = 10)

findAssocs(dtm, terms = "aluminium", corlimit = 0.3)

#The frequency table of words
head(d, 10)
#Plot word frequencies (first 10)
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word, 
        col = "lightblue", main = "Most Frequent Words",
        ylab = "Word Frequencies")



```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
